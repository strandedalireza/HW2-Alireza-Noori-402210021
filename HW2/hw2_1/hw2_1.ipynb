{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i7I7O1kv0S21"
   },
   "outputs": [],
   "source": [
    "# Feel free to import any other libraries and modules.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeXRppkryQBV"
   },
   "source": [
    "# Part One: Optimization Algorithms\n",
    "## 1. Gradient Descent\n",
    "The Gradient Descent (GD) algorithm finds the minimum of a given\n",
    "function by taking small steps along the function's gradient:\n",
    "\n",
    ">$\\Theta \\leftarrow \\Theta_0$\n",
    "\n",
    ">**while** stop condition not met **do**\n",
    "\n",
    ">$~~~~$$\\Theta \\leftarrow \\Theta - \\alpha \\nabla_\\Theta f(\\Theta)$\n",
    "\n",
    ">**end while**\n",
    "\n",
    "where $f$ is the function to minimize, $\\nabla_\\Theta f(\\Theta)$\n",
    "denotes $f$'s gradient at $\\Theta$ and $\\alpha$ is the learning rate.\n",
    "\n",
    "**Task1:** Implement the GD algorithm as a function:\n",
    "\n",
    "  \\begin{equation}\n",
    "      \\Theta_{opt} = \\text{GD}(f, \\Theta_0, \\alpha, \\rho)\n",
    "  \\end{equation}\n",
    "where $f$ is a function returning the cost and its gradient with respect to parameter vector $\\Theta$, $\\Theta_0$ is the initial value, and $\\alpha$\n",
    "is the learning rate. You can assume that $\\alpha$. remains constant during the optimization. $\\rho$ is stop condition. \\\\\n",
    "Then, use the GD algorithm to find the optimum of the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function) (Consider $a=1, b=100$).\n",
    " \\\\\n",
    "Also, plot the values found by GD at subsequent iterations.\n",
    "\n",
    "## 2. Newton's Method\n",
    "Newton's method is an iterative optimization algorithm used to find the minimum of a function.\n",
    "The basic update step in Newton's method is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Theta = \\Theta - H^{-1} \\cdot \\nabla f(\\Theta)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Where  $ H $ is the Hessian matrix of the function at $ \\Theta $, and $ H^{-1} $ is the inverse of the Hessian matrix, used to adjust the step size and direction more accurately than just using the gradient alone (as done in gradient descent).\n",
    "\n",
    "#### Line Search\n",
    "Sometimes, Newton's method may take too large of a step, which can lead to divergence. To prevent this, a simple **line search** is used. This reduces the step size $ \\alpha $ if the function value doesn't improve after the step.\n",
    "\n",
    "Steps:\n",
    "1. Compute the gradient and Hessian matrix at the current point.\n",
    "2. Calculate the step direction by multiplying the inverse of the Hessian with the gradient.\n",
    "3. Update the point by subtracting the step from the current point.\n",
    "4. If the function value doesn't improve, reduce the step size $ \\alpha $.\n",
    "5. Repeat until the gradient becomes sufficiently small (close to zero), indicating convergence.\n",
    "\n",
    "\n",
    "\n",
    "**Task2:** Implement Newton's method and compare it with the gradient descent. You will also need to implement a line search alogithm, e.g. (https://en.wikipedia.org/wiki/Backtracking_line_search) and make sure that the Newton's direction is indeed one along which the function is minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DIsMJPFs-kLd"
   },
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"Returns the value and gradient of Rosenbrock's function at x: 2d vector\"\"\"\n",
    "    a = 1\n",
    "    b = 100\n",
    "    val = (a - x[0])**2 + b * (x[1] - x[0]**2)**2\n",
    "    dv_dx0 = -2 * (a - x[0]) - 4 * b * x[0] * (x[1] - x[0]**2)\n",
    "    dv_dx1 = 2 * b * (x[1] - x[0]**2)\n",
    "    grad = np.array([dv_dx0, dv_dx1])\n",
    "\n",
    "    return val, grad\n",
    "\n",
    "def rosenbrock_hessian(x):\n",
    "    \"\"\"Returns the value, gradient and hessian of Rosenbrock's function at x: 2d vector\"\"\"\n",
    "    val, grad = rosenbrock(x)\n",
    "    a = 1\n",
    "    b = 100\n",
    "    # Hessian matrix components\n",
    "    d2v_dx0x0 = 2 - 4 * b * (x[1] - x[0]**2) + 8 * b * x[0]**2\n",
    "    d2v_dx0x1 = -4 * b * x[0]\n",
    "    d2v_dx1x0 = -4 * b * x[0]\n",
    "    d2v_dx1x1 = 2 * b\n",
    "\n",
    "    # Hessian matrix\n",
    "    hessian = np.array([[d2v_dx0x0, d2v_dx0x1],\n",
    "                [d2v_dx1x0, d2v_dx1x1]])\n",
    "\n",
    "    return val, grad, hessian\n",
    "\n",
    "\n",
    "\n",
    "def GD(f, theta0, alpha, stop_tolerance=1e-10, max_steps=1000000):\n",
    "    \"\"\"Runs gradient descent algorithm on f.\n",
    "\n",
    "    Args:\n",
    "        f: function that when evaluated on a Theta of same dtype and shape as Theta0\n",
    "            returns a tuple (value, dv_dtheta) with dValuedTheta of the same shape\n",
    "            as Theta\n",
    "        theta0: starting point\n",
    "        alpha: step length\n",
    "        stop_tolerance: stop iterations when improvement is below this threshold\n",
    "        max_steps: maximum number of steps\n",
    "    Returns:\n",
    "        tuple:\n",
    "        - theta: optimum theta found by the algorithm\n",
    "        - history: list of length num_steps containing tuples (theta, (val, dv_dtheta: np.array))\n",
    "\n",
    "    \"\"\"\n",
    "    history = []\n",
    "\n",
    "    theta = theta0\n",
    "\n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "        val, grad = f(theta)\n",
    "        theta_new = theta - alpha * grad\n",
    "        history.append([theta, (val, grad)])\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(theta_new - theta) < stop_tolerance:\n",
    "            break\n",
    "\n",
    "        theta = theta_new\n",
    "        step += 1\n",
    "\n",
    "    return theta, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "yyb5unZNaSFq",
    "outputId": "e7a90223-7227-48c2-848f-7eeb998a1af8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found optimum at [0.99999989 0.99999978] in 37027 steps (true minimum is at [1,1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the optimum of Rosenbrock function\n",
    "X0 = [0., 2.]\n",
    "Xopt, Xhist = GD(rosenbrock, X0, alpha=1e-3, stop_tolerance=1e-10, max_steps=1e6)\n",
    "\n",
    "print(\"Found optimum at %s in %d steps (true minimum is at [1,1])\" % (Xopt, len(Xhist)))\n",
    "\n",
    "# Plot how the value changes over iterations\n",
    "values = []\n",
    "for theta, (val, _) in Xhist:\n",
    "    values.append(val)\n",
    "\n",
    "plt.stem(values)\n",
    "plt.xlabel('Iter')\n",
    "plt.ylabel('Function Value')\n",
    "plt.title('how the value changes over iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQ1Bf6sNMT_B",
    "outputId": "d8e8a964-4bf1-4d88-a831-de53f9c33a56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found optimum at [1. 1.] (true minimum is at [1,1])\n"
     ]
    }
   ],
   "source": [
    "# Newton's Method\n",
    "def Newton(f, theta0, alpha=1, stop_tolerance=1e-10, max_steps=1000000):\n",
    "    \"\"\"Performs Newton's optimization method with a simple line search.\n",
    "\n",
    "    Args:\n",
    "        f: function that when evaluated on a Theta of same dtype and shape as Theta0\n",
    "            returns a tuple (value, gradient, hessian), where gradient and Hessian\n",
    "            have the same shape as Theta.\n",
    "        theta0: starting point.\n",
    "        alpha: step length for backtracking line search (default is 1).\n",
    "        stop_tolerance: stop iterations when the norm of the gradient is below this threshold.\n",
    "        max_steps: maximum number of iterations.\n",
    "    Returns:\n",
    "        tuple:\n",
    "        - theta: optimal Theta after convergence or maximum steps.\n",
    "        - history: list of tuples (theta, value, gradient) containing the optimization path.\n",
    "    \"\"\"\n",
    "    theta = theta0\n",
    "    history = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        value, grad, hess = f(theta)\n",
    "        history.append((theta, value, grad))\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < stop_tolerance:\n",
    "            break\n",
    "\n",
    "        # Line search (backtracking)\n",
    "        t = alpha\n",
    "        while f(theta - t * np.linalg.inv(hess) @ grad)[0] > value:\n",
    "            t *= 0.5\n",
    "\n",
    "        # Update step\n",
    "        theta = theta - t * np.linalg.inv(hess) @ grad\n",
    "\n",
    "    return theta, history\n",
    "\n",
    "# Test Newton's method on the Rosenbrock function\n",
    "X0 = [0., 2.]  # Initial guess\n",
    "Xopt, Xhist = Newton(rosenbrock_hessian, X0)\n",
    "\n",
    "print(\"Found optimum at %s (true minimum is at [1,1])\" % Xopt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eq7bujkni1d-"
   },
   "source": [
    "**Comparing Newton and GD**: Newton is faster, but it does not have good global convergence. On the other hand, GD is slower, but it definitely converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNXtvq8u-JSo"
   },
   "source": [
    "# Part two: MLP for MNIST Classification\n",
    "In this part, we are going to use `PyTorch`. If you want to become more familiar with it, check this resource: https://www.learnpytorch.io/\n",
    "\n",
    "#### In this homework, you need to\n",
    "- implement SGD optimizer (`./optimizer.py`)\n",
    "- implement forward and backward for FCLayer (`layers.py`)\n",
    "- implement forward and backward for SigmoidLayer (`layers.py`)\n",
    "- implement forward and backward for ReLULayer (`layers.py`)\n",
    "- implement forward and backward for DropoutLayer (`layers.py`)\n",
    "- implement train and test process (`solver.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zWtm2k5Ir-XQ"
   },
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "from solver import train, test\n",
    "from optimizer import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "NRwL2rM0O92Z",
    "outputId": "4ccb9486-af93-4561-d22e-1a273647d1ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bCXcnjSYaR-R"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "\n",
    "# Converts PIL image to tensor and scales to [0, 1] and flatten it\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1)),  # Flatten the image to a 1D tensor\n",
    "])\n",
    "\n",
    "# Load MNIST dataset with the defined transform\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Labels are already in integer form, no need for one-hot encoding\n",
    "y_train = [label for _, label in train_dataset]  # Keep labels as integers\n",
    "y_test = [label for _, label in test_dataset]  # Keep labels as integers\n",
    "\n",
    "# Convert the data into tensor datasets for training and testing\n",
    "train_dataset = TensorDataset(torch.stack([img for img, _ in train_dataset]), torch.tensor(y_train))\n",
    "test_dataset = TensorDataset(torch.stack([img for img, _ in test_dataset]), torch.tensor(y_test))\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkRty2cM-kIt",
    "outputId": "9141c330-4fc5-48da-d6a0-bd5ca037e3b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0] Average training loss: 0.6420, Average training accuracy: 0.8405\n",
      "\n",
      "Epoch [1] Average training loss: 0.3355, Average training accuracy: 0.9063\n",
      "\n",
      "Epoch [2] Average training loss: 0.2881, Average training accuracy: 0.9185\n",
      "\n",
      "Epoch [3] Average training loss: 0.2591, Average training accuracy: 0.9266\n",
      "\n",
      "Epoch [4] Average training loss: 0.2373, Average training accuracy: 0.9333\n",
      "\n",
      "Epoch [5] Average training loss: 0.2199, Average training accuracy: 0.9386\n",
      "\n",
      "Epoch [6] Average training loss: 0.2046, Average training accuracy: 0.9425\n",
      "\n",
      "Epoch [7] Average training loss: 0.1919, Average training accuracy: 0.9459\n",
      "\n",
      "Epoch [8] Average training loss: 0.1806, Average training accuracy: 0.9490\n",
      "\n",
      "Epoch [9] Average training loss: 0.1706, Average training accuracy: 0.9521\n",
      "\n",
      "Epoch [10] Average training loss: 0.1615, Average training accuracy: 0.9552\n",
      "\n",
      "Epoch [11] Average training loss: 0.1534, Average training accuracy: 0.9569\n",
      "\n",
      "Epoch [12] Average training loss: 0.1462, Average training accuracy: 0.9591\n",
      "\n",
      "Epoch [13] Average training loss: 0.1394, Average training accuracy: 0.9613\n",
      "\n",
      "Epoch [14] Average training loss: 0.1333, Average training accuracy: 0.9625\n",
      "\n",
      "Epoch [15] Average training loss: 0.1276, Average training accuracy: 0.9644\n",
      "\n",
      "Epoch [16] Average training loss: 0.1225, Average training accuracy: 0.9658\n",
      "\n",
      "Epoch [17] Average training loss: 0.1176, Average training accuracy: 0.9674\n",
      "\n",
      "Epoch [18] Average training loss: 0.1133, Average training accuracy: 0.9685\n",
      "\n",
      "Epoch [19] Average training loss: 0.1093, Average training accuracy: 0.9701\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs and learning rate\n",
    "num_epoch = 20\n",
    "\n",
    "# Use CrossEntropyLoss for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Build MLP with FCLayer and SigmoidLayer you've implemented in layers.py\n",
    "sigmoidMLP = nn.Sequential(\n",
    "    FCLayer(784, 128),  # Input layer: 784 (28x28 images) to 128 neurons\n",
    "    ReLULayer(),  # Add ReLU activation after the first layer\n",
    "    FCLayer(128, 10)  # Output layer: 128 neurons to 10 classes (for MNIST)\n",
    ")\n",
    "\n",
    "# Initialize the optimizer you've implemented in optimizer.py\n",
    "sgd = SGD(params=sigmoidMLP.parameters(), learning_rate=0.01)\n",
    "\n",
    "# Train the model using the train function you've implemented in solver.py\n",
    "sigmoidMLP = train(sigmoidMLP, criterion, sgd, train_dataloader, num_epoch, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0otZUFrr7WW",
    "outputId": "2a69247d-97b7-493d-d21a-5ef27ba73cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 0.9657.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your model using test function you've implemented in solver.py\n",
    "test(sigmoidMLP, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-2V-B_er7T2",
    "outputId": "3df1bada-025d-4dbe-8f0a-85f4e23d0e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0] Average training loss: 0.6409, Average training accuracy: 0.8378\n",
      "\n",
      "Epoch [1] Average training loss: 0.3402, Average training accuracy: 0.9050\n",
      "\n",
      "Epoch [2] Average training loss: 0.2917, Average training accuracy: 0.9187\n",
      "\n",
      "Epoch [3] Average training loss: 0.2612, Average training accuracy: 0.9274\n",
      "\n",
      "Epoch [4] Average training loss: 0.2381, Average training accuracy: 0.9335\n",
      "\n",
      "Epoch [5] Average training loss: 0.2193, Average training accuracy: 0.9390\n",
      "\n",
      "Epoch [6] Average training loss: 0.2037, Average training accuracy: 0.9436\n",
      "\n",
      "Epoch [7] Average training loss: 0.1897, Average training accuracy: 0.9468\n",
      "\n",
      "Epoch [8] Average training loss: 0.1780, Average training accuracy: 0.9504\n",
      "\n",
      "Epoch [9] Average training loss: 0.1675, Average training accuracy: 0.9536\n",
      "\n",
      "Epoch [10] Average training loss: 0.1583, Average training accuracy: 0.9562\n",
      "\n",
      "Epoch [11] Average training loss: 0.1500, Average training accuracy: 0.9585\n",
      "\n",
      "Epoch [12] Average training loss: 0.1427, Average training accuracy: 0.9604\n",
      "\n",
      "Epoch [13] Average training loss: 0.1361, Average training accuracy: 0.9627\n",
      "\n",
      "Epoch [14] Average training loss: 0.1302, Average training accuracy: 0.9643\n",
      "\n",
      "Epoch [15] Average training loss: 0.1245, Average training accuracy: 0.9659\n",
      "\n",
      "Epoch [16] Average training loss: 0.1197, Average training accuracy: 0.9675\n",
      "\n",
      "Epoch [17] Average training loss: 0.1149, Average training accuracy: 0.9688\n",
      "\n",
      "Epoch [18] Average training loss: 0.1106, Average training accuracy: 0.9701\n",
      "\n",
      "Epoch [19] Average training loss: 0.1067, Average training accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "# Build MLP with FCLayer and ReLULayer\n",
    "reluMLP = nn.Sequential(\n",
    "    FCLayer(784, 128),\n",
    "    ReLULayer(),\n",
    "    FCLayer(128, 10)\n",
    ")\n",
    "\n",
    "# Initialize optimizer\n",
    "sgd = SGD(reluMLP.parameters(), learning_rate=0.01)\n",
    "\n",
    "# Train the model\n",
    "reluMLP = train(reluMLP, criterion, sgd, train_dataloader, num_epoch, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLjxTbq8r7RZ",
    "outputId": "ba53e4ff-0860-4544-f8ec-bc169955f6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 0.9670.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test(reluMLP, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krsqZLJ-qQKH"
   },
   "source": [
    "### Overfit the model\n",
    "Try to overfit the reluMLP model. You can make the model as complex as you like, use subset of the data for training or any other approach you want.\n",
    "Then add **DropoutLayer** to your model in order to reduce overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbVFtro9qP1i",
    "outputId": "07b8845b-5bca-45d1-ab49-57d848db96e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0] Average training loss: 1.3637, Average training accuracy: 0.6260\n",
      "\n",
      "Epoch [1] Average training loss: 0.6012, Average training accuracy: 0.8301\n",
      "\n",
      "Epoch [2] Average training loss: 0.3990, Average training accuracy: 0.8877\n",
      "\n",
      "Epoch [3] Average training loss: 0.2909, Average training accuracy: 0.9189\n",
      "\n",
      "Epoch [4] Average training loss: 0.2245, Average training accuracy: 0.9424\n",
      "\n",
      "Epoch [5] Average training loss: 0.1768, Average training accuracy: 0.9541\n",
      "\n",
      "Epoch [6] Average training loss: 0.1402, Average training accuracy: 0.9727\n",
      "\n",
      "Epoch [7] Average training loss: 0.1117, Average training accuracy: 0.9795\n",
      "\n",
      "Epoch [8] Average training loss: 0.0891, Average training accuracy: 0.9873\n",
      "\n",
      "Epoch [9] Average training loss: 0.0712, Average training accuracy: 0.9941\n",
      "\n",
      "Epoch [10] Average training loss: 0.0571, Average training accuracy: 0.9971\n",
      "\n",
      "Epoch [11] Average training loss: 0.0462, Average training accuracy: 0.9990\n",
      "\n",
      "Epoch [12] Average training loss: 0.0381, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [13] Average training loss: 0.0321, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [14] Average training loss: 0.0275, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [15] Average training loss: 0.0239, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [16] Average training loss: 0.0211, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [17] Average training loss: 0.0188, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [18] Average training loss: 0.0168, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [19] Average training loss: 0.0152, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [20] Average training loss: 0.0139, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [21] Average training loss: 0.0127, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [22] Average training loss: 0.0117, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [23] Average training loss: 0.0109, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [24] Average training loss: 0.0101, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [25] Average training loss: 0.0094, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [26] Average training loss: 0.0088, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [27] Average training loss: 0.0083, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [28] Average training loss: 0.0078, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [29] Average training loss: 0.0074, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [30] Average training loss: 0.0070, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [31] Average training loss: 0.0066, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [32] Average training loss: 0.0063, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [33] Average training loss: 0.0060, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [34] Average training loss: 0.0057, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [35] Average training loss: 0.0055, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [36] Average training loss: 0.0053, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [37] Average training loss: 0.0050, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [38] Average training loss: 0.0048, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [39] Average training loss: 0.0046, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [40] Average training loss: 0.0045, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [41] Average training loss: 0.0043, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [42] Average training loss: 0.0042, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [43] Average training loss: 0.0040, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [44] Average training loss: 0.0039, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [45] Average training loss: 0.0038, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [46] Average training loss: 0.0036, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [47] Average training loss: 0.0035, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [48] Average training loss: 0.0034, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [49] Average training loss: 0.0033, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [50] Average training loss: 0.0032, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [51] Average training loss: 0.0031, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [52] Average training loss: 0.0030, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [53] Average training loss: 0.0030, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [54] Average training loss: 0.0029, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [55] Average training loss: 0.0028, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [56] Average training loss: 0.0027, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [57] Average training loss: 0.0027, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [58] Average training loss: 0.0026, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [59] Average training loss: 0.0025, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [60] Average training loss: 0.0025, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [61] Average training loss: 0.0024, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [62] Average training loss: 0.0024, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [63] Average training loss: 0.0023, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [64] Average training loss: 0.0023, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [65] Average training loss: 0.0022, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [66] Average training loss: 0.0022, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [67] Average training loss: 0.0021, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [68] Average training loss: 0.0021, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [69] Average training loss: 0.0020, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [70] Average training loss: 0.0020, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [71] Average training loss: 0.0020, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [72] Average training loss: 0.0019, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [73] Average training loss: 0.0019, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [74] Average training loss: 0.0019, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [75] Average training loss: 0.0018, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [76] Average training loss: 0.0018, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [77] Average training loss: 0.0018, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [78] Average training loss: 0.0017, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [79] Average training loss: 0.0017, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [80] Average training loss: 0.0017, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [81] Average training loss: 0.0016, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [82] Average training loss: 0.0016, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [83] Average training loss: 0.0016, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [84] Average training loss: 0.0016, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [85] Average training loss: 0.0015, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [86] Average training loss: 0.0015, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [87] Average training loss: 0.0015, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [88] Average training loss: 0.0015, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [89] Average training loss: 0.0015, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [90] Average training loss: 0.0014, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [91] Average training loss: 0.0014, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [92] Average training loss: 0.0014, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [93] Average training loss: 0.0014, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [94] Average training loss: 0.0013, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [95] Average training loss: 0.0013, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [96] Average training loss: 0.0013, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [97] Average training loss: 0.0013, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [98] Average training loss: 0.0013, Average training accuracy: 1.0000\n",
      "\n",
      "Epoch [99] Average training loss: 0.0013, Average training accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#TODO: overfit the reluMLP model\n",
    "\n",
    "# Select a small subset of the data (e.g., only 1000 samples)\n",
    "subset_train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "subset_train_dataloader = torch.utils.data.DataLoader(torch.utils.data.Subset(train_dataset, range(1000)), batch_size=32)\n",
    "\n",
    "# Increase model complexity (more layers or neurons)\n",
    "overfit_mlp = nn.Sequential(\n",
    "    FCLayer(784, 512),   # Increase the number of neurons\n",
    "    ReLULayer(),\n",
    "    FCLayer(512, 256),   # Add another hidden layer\n",
    "    ReLULayer(),\n",
    "    FCLayer(256, 10)     # Output layer\n",
    ")\n",
    "\n",
    "# Initialize the optimizer (use a high learning rate to promote overfitting)\n",
    "sgd_overfit = SGD(params=overfit_mlp.parameters(), learning_rate=0.1)\n",
    "\n",
    "# Train the model for a large number of epochs to force overfitting\n",
    "overfit_mlp = train(overfit_mlp, criterion, sgd_overfit, subset_train_dataloader, num_epoch=100, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNi_1CIHA6HX",
    "outputId": "f524c53e-04a2-4886-cb0c-ef8419267fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 0.8869.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(overfit_mlp, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeCfvh6Jr7Om",
    "outputId": "a007ce71-74da-49ba-d5be-4a3647da61b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0] Average training loss: 2.3628, Average training accuracy: 0.1104\n",
      "\n",
      "Epoch [1] Average training loss: 2.1116, Average training accuracy: 0.2334\n",
      "\n",
      "Epoch [2] Average training loss: 1.9673, Average training accuracy: 0.3398\n",
      "\n",
      "Epoch [3] Average training loss: 1.8185, Average training accuracy: 0.4375\n",
      "\n",
      "Epoch [4] Average training loss: 1.6875, Average training accuracy: 0.4990\n",
      "\n",
      "Epoch [5] Average training loss: 1.4991, Average training accuracy: 0.5645\n",
      "\n",
      "Epoch [6] Average training loss: 1.4223, Average training accuracy: 0.5840\n",
      "\n",
      "Epoch [7] Average training loss: 1.3184, Average training accuracy: 0.6094\n",
      "\n",
      "Epoch [8] Average training loss: 1.2089, Average training accuracy: 0.6543\n",
      "\n",
      "Epoch [9] Average training loss: 1.1163, Average training accuracy: 0.6758\n",
      "\n",
      "Epoch [10] Average training loss: 1.0752, Average training accuracy: 0.6846\n",
      "\n",
      "Epoch [11] Average training loss: 0.9734, Average training accuracy: 0.7207\n",
      "\n",
      "Epoch [12] Average training loss: 0.9737, Average training accuracy: 0.7148\n",
      "\n",
      "Epoch [13] Average training loss: 0.9429, Average training accuracy: 0.7129\n",
      "\n",
      "Epoch [14] Average training loss: 0.8658, Average training accuracy: 0.7451\n",
      "\n",
      "Epoch [15] Average training loss: 0.8299, Average training accuracy: 0.7598\n",
      "\n",
      "Epoch [16] Average training loss: 0.7873, Average training accuracy: 0.7646\n",
      "\n",
      "Epoch [17] Average training loss: 0.7716, Average training accuracy: 0.7588\n",
      "\n",
      "Epoch [18] Average training loss: 0.7239, Average training accuracy: 0.7891\n",
      "\n",
      "Epoch [19] Average training loss: 0.7202, Average training accuracy: 0.7881\n",
      "\n",
      "Epoch [20] Average training loss: 0.6652, Average training accuracy: 0.7920\n",
      "\n",
      "Epoch [21] Average training loss: 0.6841, Average training accuracy: 0.7832\n",
      "\n",
      "Epoch [22] Average training loss: 0.6506, Average training accuracy: 0.8096\n",
      "\n",
      "Epoch [23] Average training loss: 0.6313, Average training accuracy: 0.8027\n",
      "\n",
      "Epoch [24] Average training loss: 0.6085, Average training accuracy: 0.8223\n",
      "\n",
      "Epoch [25] Average training loss: 0.5922, Average training accuracy: 0.8213\n",
      "\n",
      "Epoch [26] Average training loss: 0.5697, Average training accuracy: 0.8291\n",
      "\n",
      "Epoch [27] Average training loss: 0.5562, Average training accuracy: 0.8262\n",
      "\n",
      "Epoch [28] Average training loss: 0.5304, Average training accuracy: 0.8447\n",
      "\n",
      "Epoch [29] Average training loss: 0.5474, Average training accuracy: 0.8330\n",
      "\n",
      "Epoch [30] Average training loss: 0.4985, Average training accuracy: 0.8535\n",
      "\n",
      "Epoch [31] Average training loss: 0.5063, Average training accuracy: 0.8418\n",
      "\n",
      "Epoch [32] Average training loss: 0.5029, Average training accuracy: 0.8545\n",
      "\n",
      "Epoch [33] Average training loss: 0.5004, Average training accuracy: 0.8496\n",
      "\n",
      "Epoch [34] Average training loss: 0.4854, Average training accuracy: 0.8652\n",
      "\n",
      "Epoch [35] Average training loss: 0.4747, Average training accuracy: 0.8662\n",
      "\n",
      "Epoch [36] Average training loss: 0.4555, Average training accuracy: 0.8643\n",
      "\n",
      "Epoch [37] Average training loss: 0.4395, Average training accuracy: 0.8721\n",
      "\n",
      "Epoch [38] Average training loss: 0.4528, Average training accuracy: 0.8633\n",
      "\n",
      "Epoch [39] Average training loss: 0.4238, Average training accuracy: 0.8633\n",
      "\n",
      "Epoch [40] Average training loss: 0.4296, Average training accuracy: 0.8701\n",
      "\n",
      "Epoch [41] Average training loss: 0.4354, Average training accuracy: 0.8750\n",
      "\n",
      "Epoch [42] Average training loss: 0.4259, Average training accuracy: 0.8652\n",
      "\n",
      "Epoch [43] Average training loss: 0.4016, Average training accuracy: 0.8799\n",
      "\n",
      "Epoch [44] Average training loss: 0.3848, Average training accuracy: 0.8828\n",
      "\n",
      "Epoch [45] Average training loss: 0.4023, Average training accuracy: 0.8760\n",
      "\n",
      "Epoch [46] Average training loss: 0.3757, Average training accuracy: 0.8838\n",
      "\n",
      "Epoch [47] Average training loss: 0.3735, Average training accuracy: 0.8975\n",
      "\n",
      "Epoch [48] Average training loss: 0.3618, Average training accuracy: 0.8867\n",
      "\n",
      "Epoch [49] Average training loss: 0.3667, Average training accuracy: 0.8926\n",
      "\n",
      "Epoch [50] Average training loss: 0.3692, Average training accuracy: 0.8828\n",
      "\n",
      "Epoch [51] Average training loss: 0.3401, Average training accuracy: 0.9023\n",
      "\n",
      "Epoch [52] Average training loss: 0.3259, Average training accuracy: 0.9004\n",
      "\n",
      "Epoch [53] Average training loss: 0.3306, Average training accuracy: 0.8994\n",
      "\n",
      "Epoch [54] Average training loss: 0.3527, Average training accuracy: 0.8955\n",
      "\n",
      "Epoch [55] Average training loss: 0.3394, Average training accuracy: 0.9004\n",
      "\n",
      "Epoch [56] Average training loss: 0.3155, Average training accuracy: 0.9092\n",
      "\n",
      "Epoch [57] Average training loss: 0.3227, Average training accuracy: 0.8975\n",
      "\n",
      "Epoch [58] Average training loss: 0.3356, Average training accuracy: 0.8994\n",
      "\n",
      "Epoch [59] Average training loss: 0.3072, Average training accuracy: 0.9141\n",
      "\n",
      "Epoch [60] Average training loss: 0.2979, Average training accuracy: 0.9170\n",
      "\n",
      "Epoch [61] Average training loss: 0.2866, Average training accuracy: 0.9160\n",
      "\n",
      "Epoch [62] Average training loss: 0.3175, Average training accuracy: 0.9121\n",
      "\n",
      "Epoch [63] Average training loss: 0.2656, Average training accuracy: 0.9189\n",
      "\n",
      "Epoch [64] Average training loss: 0.2955, Average training accuracy: 0.9062\n",
      "\n",
      "Epoch [65] Average training loss: 0.2603, Average training accuracy: 0.9297\n",
      "\n",
      "Epoch [66] Average training loss: 0.2751, Average training accuracy: 0.9160\n",
      "\n",
      "Epoch [67] Average training loss: 0.2874, Average training accuracy: 0.9023\n",
      "\n",
      "Epoch [68] Average training loss: 0.2630, Average training accuracy: 0.9150\n",
      "\n",
      "Epoch [69] Average training loss: 0.2572, Average training accuracy: 0.9277\n",
      "\n",
      "Epoch [70] Average training loss: 0.2629, Average training accuracy: 0.9297\n",
      "\n",
      "Epoch [71] Average training loss: 0.2535, Average training accuracy: 0.9209\n",
      "\n",
      "Epoch [72] Average training loss: 0.2519, Average training accuracy: 0.9287\n",
      "\n",
      "Epoch [73] Average training loss: 0.2520, Average training accuracy: 0.9209\n",
      "\n",
      "Epoch [74] Average training loss: 0.2440, Average training accuracy: 0.9316\n",
      "\n",
      "Epoch [75] Average training loss: 0.2455, Average training accuracy: 0.9355\n",
      "\n",
      "Epoch [76] Average training loss: 0.2259, Average training accuracy: 0.9229\n",
      "\n",
      "Epoch [77] Average training loss: 0.2351, Average training accuracy: 0.9287\n",
      "\n",
      "Epoch [78] Average training loss: 0.2412, Average training accuracy: 0.9355\n",
      "\n",
      "Epoch [79] Average training loss: 0.2370, Average training accuracy: 0.9277\n",
      "\n",
      "Epoch [80] Average training loss: 0.2351, Average training accuracy: 0.9326\n",
      "\n",
      "Epoch [81] Average training loss: 0.2250, Average training accuracy: 0.9346\n",
      "\n",
      "Epoch [82] Average training loss: 0.2118, Average training accuracy: 0.9395\n",
      "\n",
      "Epoch [83] Average training loss: 0.2061, Average training accuracy: 0.9463\n",
      "\n",
      "Epoch [84] Average training loss: 0.2034, Average training accuracy: 0.9463\n",
      "\n",
      "Epoch [85] Average training loss: 0.2228, Average training accuracy: 0.9336\n",
      "\n",
      "Epoch [86] Average training loss: 0.2194, Average training accuracy: 0.9355\n",
      "\n",
      "Epoch [87] Average training loss: 0.2254, Average training accuracy: 0.9346\n",
      "\n",
      "Epoch [88] Average training loss: 0.1956, Average training accuracy: 0.9512\n",
      "\n",
      "Epoch [89] Average training loss: 0.1979, Average training accuracy: 0.9424\n",
      "\n",
      "Epoch [90] Average training loss: 0.2313, Average training accuracy: 0.9258\n",
      "\n",
      "Epoch [91] Average training loss: 0.2113, Average training accuracy: 0.9365\n",
      "\n",
      "Epoch [92] Average training loss: 0.1870, Average training accuracy: 0.9443\n",
      "\n",
      "Epoch [93] Average training loss: 0.2145, Average training accuracy: 0.9307\n",
      "\n",
      "Epoch [94] Average training loss: 0.1964, Average training accuracy: 0.9404\n",
      "\n",
      "Epoch [95] Average training loss: 0.1977, Average training accuracy: 0.9375\n",
      "\n",
      "Epoch [96] Average training loss: 0.1974, Average training accuracy: 0.9443\n",
      "\n",
      "Epoch [97] Average training loss: 0.1923, Average training accuracy: 0.9512\n",
      "\n",
      "Epoch [98] Average training loss: 0.1697, Average training accuracy: 0.9521\n",
      "\n",
      "Epoch [99] Average training loss: 0.1913, Average training accuracy: 0.9463\n"
     ]
    }
   ],
   "source": [
    "from layers import DropoutLayer\n",
    "\n",
    "#TODO: add DropoutLayer to your model\n",
    "dropout_mlp = nn.Sequential(\n",
    "    FCLayer(784, 512),\n",
    "    ReLULayer(),\n",
    "    DropoutLayer(0.5),  # Add dropout with a 50% probability\n",
    "    FCLayer(512, 256),\n",
    "    ReLULayer(),\n",
    "    DropoutLayer(0.5),  # Add another dropout layer\n",
    "    FCLayer(256, 10)\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "sgd_dropout = SGD(params=dropout_mlp.parameters(), learning_rate=0.01)\n",
    "\n",
    "# Train the model with dropout to reduce overfitting\n",
    "dropout_mlp = train(dropout_mlp, criterion, sgd_dropout, subset_train_dataloader, num_epoch=100, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UShs-KYlr7L4",
    "outputId": "b46eb144-87e7-463a-d77c-fc22958a9389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 0.8878.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(dropout_mlp, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
